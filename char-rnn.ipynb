{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generování textu znakovou RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tqdm\n",
    "\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ans\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V tomto cvičení nebudeme používat GPU, protože budeme zpracovávat znaky po jednom a v takto malých dávkách overhead způsobený neustálými přesuny dat mezi GPU a RAM výpočty pouze zpomalí."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Namísto obrazu tentokrát použijeme textová data. Konkrétně se jedná o novinové nadpisy, které se budeme snažit generovat automaticky. Všechna data jsou v jediném souboru, který si stáhněte [odsud](https://1drv.ms/t/s!AotVPA94wWKxoWLULaBqvPXiNS5t) a uložte jako `data/headlines.txt`.\n",
    "\n",
    "Z textu byly odstraneny hacky, carky a vsechny nestandardni znaky. Neni tedy potreba resit kodovani apod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('data/headlines.txt').read()\n",
    "lines = [line.strip() for line in data.split('\\n') if line]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ukázka dat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(i, random.choice(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sada znaků = náš slovník:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(sorted(set(data)))\n",
    "print(len(chars), chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Následující tabulka (`dict`) nám usnadní převod znaku na index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr2idx = {c: i for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podíváme se na statistické rozložení prvních znaků ve větách."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {c: 0 for c in chars}\n",
    "for line in lines:\n",
    "    counts[line[0]] += 1\n",
    "counts = np.array([counts[c] for c in chars], dtype=np.float)\n",
    "p0 = counts / counts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "rects = plt.bar(range(len(chars)), 100. * p0)\n",
    "plt.xticks(range(len(chars)), ['{}'.format(repr(c)) for c in chars])\n",
    "for r in rects:\n",
    "    x, w, h = r.get_x(), r.get_width(), r.get_height()\n",
    "    plt.text(x + w / 2., h + 0.1, '{:.1f}'.format(h), ha='center', va='bottom', fontsize=8)\n",
    "plt.ylabel('počet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sekvenční data a PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Následující funkce převede řetězec na sekvenci čísel odpovídajících indexům znaků v tabulce. Pokud např. `chars = ['a', 'b', 'c']`, pak řetězec `'acba'` převede na `[0, 2, 1, 0]`. Výsledek vrátí jako PyTorch `Variable`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = chr2idx[string[c]]\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = char_tensor('abca')\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Další funkce bude dělat opak: převede sekvenci indexů na řetězec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_string(indices):\n",
    "    if isinstance(indices, torch.Tensor):\n",
    "        indices = indices.data\n",
    "    return ''.join([chars[i] for i in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_string(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekvenci čísel potřebujeme převést na vektory jednotlivých znaků. Tento proces se v anglické literatuře označuje jako embedding a PyTorch ho implementuje jako vrstvu třídou `Embedding`. Vyjádřením této operace diferencovatelnou vrstvou umožňuje učení vektorů, které tedy nemusejí být fixní. O tom ale až příště."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# velikost slovniku je `len(chars)`\n",
    "# dimenze znakoveho vektoru bude napr. 30\n",
    "emb = nn.Embedding(len(chars), 30)\n",
    "\n",
    "# dopredny pruchod\n",
    "e = emb(x)\n",
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Embedding` nedělá nic jiného, než že na výstup pro znak s indexem $i$ vrátí $i$-tý řádek své váhové matice `weight`, která drží vektory slov. Defaultně je tato matice inicializována náhodně. Pokud první písmeno v příkladu bylo 'a', jehož index ve \"slovníku\" `chars` je 12, první řádek embeddingu `e` bude odpovídat 13. řádku (index 12) matice `emb.weight`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool(torch.all(e[0] == emb.weight[12]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN v PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch implementuje tři z nejrozšířenějších typů sítí třídami `RNN`, `LSTM` a `GRU`. API je pro všechny stejné: dopředný průchod `forward` očekává \"zespodu\" nějaký vstup `input` a \"zleva\" minulý stav `h0`. U `LSTM` je tento stav dvouvektorový. Výstupem je `output`, což je vlastně sekvence skrytých stavů poslední vrstvy rekurentní sítě pro jednotlivé kroky v čase, a nový stav `hn` po provedení celého průchodu. Vše vystihuje následující obrázek.\n",
    "\n",
    "![](https://i.stack.imgur.com/SjnTl.png)\n",
    "\n",
    "Zdroj: https://stackoverflow.com/a/48305882/9418551\n",
    "\n",
    "V nejjednoušším případě máme pouze jednu vrstvu sítě a jeden krok. Potom `output` a `hn` jsou stejné. `output` tedy **neprochází žádnou lineární vrstvou**, jak by se mohlo na první pohled zdát. Transformaci na skóre/pravděpodobnost jednotlivých znaků tedy musíme provést sami.\n",
    "\n",
    "**Příklad:** porovnejme `output` a `hidden`.\n",
    "tensory by měly být tvaru `(seq, batch, dim)`\n",
    "- `seq` ... jak jdou znaky ve \"věte\" za sebou\n",
    "- `batch` ... počet paralelně zpracovávaných sekvencí, nezávisle na sobě\n",
    "- `dim` ... příznaky na vstupu\n",
    "\n",
    "Například tedy: `(10, 3, 5)` by znamenalo:\n",
    "- 3 paralelně zpracovávané\n",
    "- 10-znakové věty,\n",
    "- kde každý znak reprezentuje 5dimenzionální vektor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do site posleme pouze jeden znak\n",
    "e0 = e[0].reshape(1, 1, -1)\n",
    "e0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN ocekava na vstupu vektor o rozmeru 6 a skryty stav bude mit rozmer 8\n",
    "rnn = nn.RNN(30, 8)\n",
    "\n",
    "# inicializace skryteho stavu a vstupu\n",
    "# tensory by mely byt tvaru (seq, batch, dim)\n",
    "h = torch.rand(8)\n",
    "o, h = rnn(e0)\n",
    "\n",
    "print(o)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nyní už více samostatně. Zadefinujeme vlastní třídu, která bude řešit jednotlivé kroky sama ve svém dopředném průchodu. Vstupem tedy bude sekvence čísel, výstupem skóre jednotlivých kroků a skrytý stav z posledního kroku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, voc_size, emb_dim, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        #################################################################\n",
    "        # ZDE DOPLNIT\n",
    "        \n",
    "        self.emb = ...\n",
    "        self.rnn = ...\n",
    "        self.fc = ...\n",
    "        \n",
    "        #################################################################\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        #################################################################\n",
    "        # ZDE DOPLNIT\n",
    "        \n",
    "        ...\n",
    "        \n",
    "        #################################################################\n",
    "        \n",
    "        return score, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        \n",
    "        #################################################################\n",
    "        # ZDE DOPLNIT\n",
    "        # funkce vrati skryty vektor nainicalizovany na nuly\n",
    "        \n",
    "        ...\n",
    "        \n",
    "        #################################################################\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# ZDE DOPLNIT\n",
    "\n",
    "voc_size = ...\n",
    "emb_dim = ...\n",
    "hidden_dim = ...\n",
    "output_dim = ...\n",
    "\n",
    "#################################################################\n",
    "\n",
    "rnn = RNN(voc_size, emb_dim, hidden_dim, output_dim, n_layers=1)\n",
    "stats = ans.Stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vytvoříme si také funkci pro samplování z naší sítě. Funkce přijme model `rnn`, nějaký inicializační text `init_text`, příp. i inicializační `hidden`, a vygeneruje text - vrací tedy string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(rnn, init_text='', hidden=None, maxlen=150, mode='multinomial', temperature=0.6):\n",
    "    \"\"\"\n",
    "    generuje text pomoci modelu `rnn`\n",
    "    \n",
    "    vstupy:\n",
    "        rnn ... rekurentni sit odvozena z `nn.Module`, ktera po zavolani vraci dvojici (vyst_skore, skryta_rep)\n",
    "        init_text ... inicializacni text, na ktery generovani textu navaze\n",
    "        hidden ... inicializace skryte reprezentace\n",
    "        maxlen ... maximalni delka generovaneho textu\n",
    "        mode ... zpusob vyberu nasledujiciho znaku, viz komentare v kodu\n",
    "        temperature ... vyhlazeni multinomialniho rozlozeni, viz komentare v kodu\n",
    "    \"\"\"\n",
    "    # vystupni text bude pole (na konci prevedeme zpet na str)\n",
    "    out_text = list(init_text)\n",
    "    \n",
    "    # pokud nezadan, inicializujeme nahodne, dle rozlozeni prvnich znaku\n",
    "    if not out_text:\n",
    "        s = np.random.choice(len(chars), p=p0)\n",
    "        out_text = [chars[s]]\n",
    "    \n",
    "    # to same hidden\n",
    "    if hidden is None:\n",
    "        hidden = rnn.init_hidden()\n",
    "        \n",
    "        # vstup projedeme siti, abychom ziskali aktualni hidden stav\n",
    "        x = char_tensor(out_text)\n",
    "        for i in range(len(out_text)):\n",
    "            score, hidden = rnn(x[i], hidden)\n",
    "    \n",
    "    # nasledujici znak je posledni znak prozatimniho vystupu\n",
    "    x = char_tensor(out_text[-1])\n",
    "    \n",
    "    # pravdepodobnosti muzeme pocitat softmaxem\n",
    "    softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    while True:\n",
    "        # dopredny pruchod\n",
    "        score, hidden = rnn(x, hidden)\n",
    "        \n",
    "        # pravdepodobnosti znaku\n",
    "        p = softmax(score).detach().numpy().squeeze()\n",
    "        \n",
    "        # vyberem index `k` nasleduciho znaku\n",
    "        if mode == 'multinomial':\n",
    "            # nasledujici znak bude vybran dle ad hoc multinomialniho rozlozeni\n",
    "            # parametr `temperature` ... vyssi hodnota znamena nahodnejsi vysledky\n",
    "            # viz https://github.com/karpathy/char-rnn#sampling\n",
    "            k = torch.multinomial(score.view(-1).div(temperature).exp(), 1)[0]\n",
    "        elif mode == 'argmax':\n",
    "            #################################################################\n",
    "            # ZDE DOPLNIT\n",
    "            \n",
    "            # nasledujici znak bude ten, jehoz pravdepodobnost vysla maximalni\n",
    "            k = ...\n",
    "            \n",
    "            #################################################################\n",
    "        elif mode == 'proportional':\n",
    "            #################################################################\n",
    "            # ZDE DOPLNIT\n",
    "            \n",
    "            # nasl. znak se vybere nahodne, ale s pravdepodobnosti proporcionalni k vystupu softmaxu\n",
    "            # napr. pokud znak 'x' ma dle softmaxu 84 %, bude s pravdepodobnosti 84 % vybran jako vstup do dalsi iterace\n",
    "            k = ...\n",
    "            \n",
    "            #################################################################\n",
    "        \n",
    "        #################################################################\n",
    "        # ZDE DOPLNIT\n",
    "        \n",
    "        # zastavit, pokud end-token\n",
    "        ...\n",
    "        \n",
    "        # pridat znak na vystup\n",
    "        ...\n",
    "        \n",
    "        # zastavit, pokud text je moc dlouhy\n",
    "        ...\n",
    "        \n",
    "        # pripravit vstupni vektor `x` pro dalsi iteraci\n",
    "        ...\n",
    "        \n",
    "        #################################################################\n",
    "    \n",
    "    return ''.join(out_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(sample(rnn, init_text='prezident', mode='multinomial'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trénování"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V každé iteraci pomocí funkce `char_tensor` vytvoříme trénovací data $x_i$, $y_i$, což budou číselné indexy znaků tak, jak je definuje tabulka `chr2idx`. Budeme trénovat generování znaků, tzn. že požadovaným výstupem $y_i$ (label, target) pro vstup $x_i$ je vždy následující znak $y_i=x_{i+1}$. Vektor `y_train` je tedy v tomto případě stejného rozměru jako `X_train`. Poslední znak má jako label `\\n`, značící konec sekvence.\n",
    "\n",
    "Vyzkoušejte si na příkladu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = random.choice(lines)\n",
    "print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# ZDE DOPLNIT\n",
    "\n",
    "x = ...\n",
    "y = ...\n",
    "\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('data:   {} ... {}'.format(to_string(x[:10]), to_string(x[-10:])))\n",
    "print('label:  {} ... {}'.format(to_string(y[:10]), to_string(y[-10:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=...)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "example = sample(rnn, mode='argmax')\n",
    "max_per_epoch = ...\n",
    "\n",
    "for epoch in range(1):\n",
    "    # data budou nahodne prehazena\n",
    "    train_ids = np.random.permutation(len(lines))[:max_per_epoch]\n",
    "\n",
    "    # progressbar\n",
    "    pb = tqdm.tqdm_notebook(train_ids, desc='ep {:03d}'.format(epoch))\n",
    "    \n",
    "    stats.new_epoch()\n",
    "    \n",
    "    for it, idx in enumerate(pb):\n",
    "        hidden = rnn.init_hidden()\n",
    "        rnn.zero_grad()\n",
    "        loss = 0.\n",
    "    \n",
    "        #################################################################\n",
    "        # ZDE DOPLNIT\n",
    "        \n",
    "        x = ...\n",
    "        y = ...\n",
    "        \n",
    "        for ic, c in enumerate(lines[idx]):\n",
    "            # dopredny pruchod pro `ic`-ty znak\n",
    "            ...\n",
    "            loss += ...\n",
    "        \n",
    "        loss /= len(x)\n",
    "        \n",
    "        #################################################################\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if it % 100 == 0:\n",
    "            example = sample(rnn)\n",
    "        \n",
    "        stats.append_batch_stats('train', loss=float(loss))\n",
    "        pb.set_postfix(loss='{:.3f}'.format(stats.ravg('train', 'loss')), ex=example[:40])\n",
    "    \n",
    "# pripadne ulozit model\n",
    "# torch.save(rnn.state_dict(), f'lstm-{epoch:02d}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.plot_by_batch(block_len=10, right_metric=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(sample(rnn, init_text='prezident', mode='multinomial'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
